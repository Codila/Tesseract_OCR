{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Murphy_1.PNG', 'Murphy_2.PNG', 'Murphy_3.PNG', 'Murphy_4.PNG', 'Murphy_5.PNG', 'Murphy_6.PNG', 'Murphy_7.PNG', 'Murphy_8.PNG', 'Murphy_9.PNG', 'Murphy_10.PNG', 'Murphy_11.PNG', 'Murphy_12.PNG', 'Murphy_13.PNG', 'Murphy_14.PNG', 'Murphy_15.PNG', 'Murphy_16.PNG', 'Murphy_17.PNG', 'Murphy_18.PNG', 'Murphy_19.PNG', 'Murphy_20.PNG', 'Murphy_21.PNG', 'Murphy_22.PNG', 'Murphy_23.PNG', 'Murphy_24.PNG', 'Murphy_25.PNG', 'Murphy_26.PNG']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import os \n",
    "path=os.listdir(\"png/\")\n",
    "\n",
    "def main(chunk):\n",
    "      return int(chunk) if chunk.isdigit() else chunk\n",
    "def natural_keys(text):\n",
    "       return [  main(element) for element in re.split('_(\\d+)', text) ] # for each element split on each element of a text\n",
    "\n",
    "    \n",
    "if  __name__==\"_main_\":\n",
    "    main(chunk)\n",
    "    natural_keys(text)\n",
    "    \n",
    "    \n",
    "path.sort(key=natural_keys)\n",
    "print(path)   # sorting the path by page numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Introduction\n",
      "\n",
      "With the ever increasing amounts of data in electronic form, the need for automated methods\n",
      "for data analysis continues to grow. The goal of machine leaming is to develop methods that\n",
      "can automatically detect pattems in data, and then to use the uncovered patterns to predict\n",
      "future data or other outcomes of interest. Machine learning is thus closely related to the fields\n",
      "of statistics and data mining, but differs slightly in terms of its emphasis and terminology. This\n",
      "book provides a detailed introduction to the field, and includes worked examples drawn from\n",
      "application domains such as molecular biology, text processing, computer vision, and robotics.\n",
      "\n",
      "Target audience\n",
      "\n",
      "This book is suitable for upper-level undergraduate students and beginning graduate students\n",
      "in computer science, statistics, electrical engineering, econometrics, or anyone else who has the\n",
      "appropriate mathematical background. Specifically, the reader is assumed to already be familiar\n",
      "with basic multivariate calculus, probability, linear algebra, and computer programming. Prior\n",
      "exposure to statistics is helpful but not necessary.\n",
      "\n",
      "A probabilistic approach\n",
      "This books adopts the view that the best way to make machines that can learn from data is to\n",
      "use the tools of probability theory, which has been the mainstay of statistics and engineering for\n",
      "centuries. Probability theory can be applied to any problem involving uncertainty. In machine\n",
      "leaning, uncertainty comes in many forms: what is the best prediction (or decision) given some\n",
      "data? what is the best model given some data? what measurement should I perform next? etc.\n",
      "\n",
      "The systematic application of probabilistic reasoning to all inferential problems, including\n",
      "inferring parameters of statistical models, is sometimes called a Bayesian approach. However,\n",
      "this term tends to elicit very strong reactions (either positive or negative, depending on who\n",
      "you ask), so we prefer the more neutral term “probabilistic approach’. Besides, we will often\n",
      "use techniques such as maximum likelihood estimation, which are not Bayesian methods, but\n",
      "certainly fall within the probabilistic paradigm.\n",
      "\n",
      "Rather than describing a cookbook of different heuristic methods, this book stresses a princi-\n",
      "pled model-based approach to machine learning. For any given model, a variety of algorithms\n"
     ]
    }
   ],
   "source": [
    "# import the necessary packages\n",
    "from PIL import Image\n",
    "import pytesseract\n",
    "import argparse\n",
    "import cv2\n",
    "\n",
    "# tesseract must be in the path of the directory if not set it the /tessearact location in your working directory\n",
    "for i in range(len(path)):\n",
    "    text = pytesseract.image_to_string(Image.open(\"png/\"+path[i]))\n",
    "print(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"1.2.1.2\\n\\nreasonable guess is that blue crescent should be y = 1, since all blue shapes are labeled 1 in the\\ntraining set. The yellow circle is harder to classify, since some yellow things are labeled y = 1\\nand some are labeled y = 0, and some circles are labeled y = 1 and some y = 0. Consequently\\nit is not clear what the right label should be in the case of the yellow circle. Similarly, the\\ncorrect label for the blue arrow is unclear, since although all the previously seen blue things\\nhave a positive label, all the previously seen arrows have a negative label.\\n\\nThe need for probabilistic predictions\\n\\nTo handle ambiguous cases, such as the yellow circle above, it is desirable to return a probability.\\nThe reader is assumed to already have some familiarity with basic concepts in probability. If\\nnot, please consult Chapter 2 for a refresher, if necessary.\\n\\nWe will denote the probability distribution over possible labels, given the input vector x and\\ntraining set D by p(y|x, D). In general, this represents a vector of length C. (If there are just two\\nclasses, it is sufficient to return the single number p(y = 1|x, D), since p(y = 1|x,D) + p(y =\\nO|x,D) = 1) In our notation, we make explicit that the probability is conditional on the test\\ninput x, as well as the training set D, by putting these terms on the right hand side of the\\nconditioning bar |. We are also implicitly conditioning on the form of model that we use to make\\npredictions. When choosing between different models, we will make this assumption explicit by\\nwriting p(y|x,D, M), where M denotes the model. However, if the model is clear from context,\\nwe will drop M from our notation for brevity.\\n\\nGiven a probabilistic output, we can always compute our “best guess” as to the “true label”\\nusing\\n\\n \\n\\na= fs) an\\n\\n \\n\\nThis corresponds to the most probable class label, and is called the mode of the distribution\\n(ylX, D); it is also known as a MAP estimate (MAP stands for maximum a posteriori). Using\\nthe most probable label makes intuitive sense, but we will give a more formal justification for\\nthis procedure in Section 5.7.\\n\\nNow consider a case such as the yellow circle, where p(g|x, D) is far from LO. If we are not\\nvery confident of our answer, it might be better to say “I don't know” instead of returning an\\nanswer that we don't really trust. This is particularly important in domains such as medicine\\nand finance where we may be risk averse, as we explain in Section 5.7.\\n\\nOne interesting application of the “I don’t know” option arises when playing the TV game\\nshow Jeopardy. In this game, contestants have to solve various word puzzles and answer a\\nvariety of trivia questions, but if they answer incorrectly, they lose money. In 2011, IBM unveiled\\na computer system called Watson which beat the top human Jeopardy champion. Watson uses a\\nvariety of interesting techniques (Ferrucci et al. 2010), but the most pertinent one for our present\\ndiscussion is that it contains a module that estimates how confident it is of its answer. The\\nsystem only chooses to “buzz in” its answer if sufficiently confident it is correct.\\n\\nAnother application where estimating uncertainty is important is in online advertising. Google\\nhas a system known as SmartA$s (ad selection system) that predicts the probability you will click\\non an ad based on vour search history and other user and ad-snecific features (Metz 2010). This\""
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = pytesseract.image_to_string(Image.open(\"png/\"+path[5])) # Formula's dont show up nicely\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
